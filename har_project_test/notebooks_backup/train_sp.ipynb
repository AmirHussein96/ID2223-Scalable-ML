{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1949</td><td>application_1512575073636_0477</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1512575073636_0477/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop22:8042/node/containerlogs/container_e26_1512575073636_0477_01_000001/har_1__kimham00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from hops import hdfs\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from tensorflowonspark import TFCluster\n",
    "from hops import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_path = \"/Projects/\" + hdfs.project_name()\n",
    "\n",
    "TRAIN_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/features\"\n",
    "TRAIN_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/labels\"\n",
    "TEST_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/features\"\n",
    "TEST_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/labels\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_args(num_executors):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', \"--cluster\", action='store_true', default=False)\n",
    "    parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\n",
    "    parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "    parser.add_argument(\"-f\", \"--features\", help=\"HDFS path to features in parallelized format\", default=TRAIN_FEATURES_PATH)\n",
    "    parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to labels in parallelized format\", default=TRAIN_LABELS_PATH)\n",
    "    parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/inference\", default=project_path + \"/HAR_Dataset/saved_model\")\n",
    "    parser.add_argument(\"-r\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=project_path + \"/HAR_Dataset/predictions\")\n",
    "    parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=1000)\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", help=\"number of records per batch\", type=int, default=100)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_data(sc, featuresFile, labelsFile):\n",
    "    labels = sc.textFile(labelsFile).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "    features = sc.textFile(featuresFile).map(lambda ln: [float(x) for x in ln.split(',')]).repartition(labels.getNumPartitions())\n",
    "    dataRDD = features.zip(labels)\n",
    "    return dataRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def map_fun(args, ctx):\n",
    "    from hops import util\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    from hops import tensorboard # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "\n",
    "    NUM_FEATURES = 8\n",
    "    NUM_CLASSES = 7\n",
    "\n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"%d: \" % worker_num)\n",
    "        print(arg)\n",
    "\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    print_log(worker_num, \"task_index: {0}, job_name {1}, cluster_spec: {2}\".format(task_index, job_name, cluster_spec))\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "    \n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 2) * 5)\n",
    "    \n",
    "    print_log(worker_num, \"tensorboard: {0}\".format(args.tensorboard))\n",
    "    worker_num = ctx.worker_num\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    print_log(worker_num, \"batch_size: {0}\".format(batch_size))\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def feed_dict(batch):\n",
    "        # Convert from [(features, labels)] to two numpy arrays of the proper type\n",
    "        features = []\n",
    "        labels = []\n",
    "        for item in batch:\n",
    "            features.append(item[0])\n",
    "            labels.append(item[1])\n",
    "        xs = np.array(features)\n",
    "        xs = xs.astype(np.float32)\n",
    "        ys = np.array(labels)\n",
    "        ys = ys.reshape(len(ys))\n",
    "        ys = ys.astype(np.uint8)\n",
    "        return (xs, ys)\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        print_log(worker_num, \"Parameter Server Joining\")\n",
    "        server.join()\n",
    "\n",
    "    elif job_name == \"worker\":\n",
    "        print_log(worker_num, \"worker starting\")\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "                worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "                cluster=cluster)):\n",
    "            global_step = tf.Variable(0)\n",
    "\n",
    "            x = tf.placeholder(tf.float32, [None, NUM_FEATURES])\n",
    "            y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "            # The model\n",
    "            W = tf.Variable(tf.zeros([NUM_FEATURES, NUM_CLASSES]))\n",
    "            tf.summary.histogram(\"hidden_weights\", W)\n",
    "            b = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "            logits = tf.matmul(x, W) + b\n",
    "\n",
    "            # Define loss and optimizer\n",
    "            cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "            tf.summary.scalar(\"loss\", cross_entropy)\n",
    "\n",
    "            train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy, global_step=global_step)\n",
    "\n",
    "            prediction = tf.argmax(tf.nn.softmax(logits), 1, name=\"prediction\")\n",
    "            # Test trained model\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), y)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"acc\", accuracy)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            #logdir = TFNode.hdfs_path(ctx, args.model)\n",
    "            logdir = tensorboard.logdir()\n",
    "            print_log(worker_num, \"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "            if job_name == \"worker\" and task_index == 0:\n",
    "                summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "            if args.mode == \"train\":\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                     logdir=logdir,\n",
    "                                     init_op=init_op,\n",
    "                                     summary_op=None,\n",
    "                                     summary_writer=None,\n",
    "                                     saver=saver,\n",
    "                                     global_step=global_step,\n",
    "                                     stop_grace_secs=300,\n",
    "                                     save_model_secs=10)\n",
    "            else:\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                     logdir=logdir,\n",
    "                                     summary_op=None,\n",
    "                                     saver=saver,\n",
    "                                     global_step=global_step,\n",
    "                                     stop_grace_secs=300,\n",
    "                                     save_model_secs=0)\n",
    "        # The supervisor takes care of session initialization, restoring from\n",
    "        # a checkpoint, and closing when done or an error occurs.\n",
    "        with sv.managed_session(server.target) as sess:\n",
    "            print_log(worker_num, \"session ready, starting training\")\n",
    "\n",
    "            # Loop until the supervisor shuts down or maximum steps have completed.\n",
    "            step = 0\n",
    "            # TFNode.DataFeed handles SPARK input mode, will convert the RDD into TF formats\n",
    "            tf_feed = TFNode.DataFeed(ctx.mgr, args.mode == \"train\")\n",
    "            while not sv.should_stop() and not tf_feed.should_stop() and step < args.steps:\n",
    "                # Run a training step asynchronously.\n",
    "                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "                # perform *synchronous* training.\n",
    "\n",
    "                # using feed_dict\n",
    "                batch_xs, batch_ys = feed_dict(tf_feed.next_batch(batch_size))\n",
    "                feed = {x: batch_xs, y: batch_ys}\n",
    "\n",
    "                if len(batch_xs) > 0:\n",
    "                    if args.mode == \"train\":\n",
    "                        _, summary, step = sess.run([train_step, summary_op, global_step], feed_dict=feed)\n",
    "                        # print accuracy and save model checkpoint to HDFS every 100 steps\n",
    "                        if (step % 100 == 0):\n",
    "                            acc = sess.run(accuracy, {x: batch_xs, y: batch_ys})\n",
    "                            print_log(worker_num, \"step: {0}, acc: {1}\".format(step, acc))\n",
    "\n",
    "                        if sv.is_chief:\n",
    "                            summary_writer.add_summary(summary, step)\n",
    "                    else:  # args.mode == \"inference\"\n",
    "                        if (len(batch_ys == batch_size)):\n",
    "                            pred, acc = sess.run([prediction, accuracy], feed_dict=feed)\n",
    "                            # acc, loss = sess.run([accuracy, cross_entropy_loss], feed_dict=feed)\n",
    "                            results = [\"Label: {0}, Prediction: {1}\".format(label, pred) for label, pred in\n",
    "                                       zip(batch_ys, pred)]\n",
    "                            print_log(worker_num, \"len results: {}\".format(len(results)))\n",
    "                            tf_feed.batch_results(results)\n",
    "                            print_log(worker_num, \"acc: {0}\".format(acc))\n",
    "                        else:\n",
    "                            print_log(worker_num, \"Skipping last batch because it is not complete\")\n",
    "\n",
    "        if sv.should_stop() or step >= args.steps:\n",
    "            print_log(worker_num, \"terminating\")\n",
    "            tf_feed.terminate()\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print_log(worker_num, \"stopping supervisor\")\n",
    "        sv.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark Cluster Setup For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_setup_cluster_training():\n",
    "    #from hops import tensorboard\n",
    "    num_executors = util.num_executors(spark)\n",
    "    num_ps = util.num_param_servers(spark)\n",
    "    args = parse_args(num_executors)\n",
    "    dataRDD = read_data(sc, args.features, args.labels)\n",
    "    #args.model = tensorboard.logdir()\n",
    "    #print(args.tensorboard)\n",
    "    cluster = TFCluster.run(sc, map_fun, args, num_executors, num_ps, args.tensorboard, TFCluster.InputMode.SPARK)\n",
    "    if args.mode == \"train\":\n",
    "        print(\"servers for training\")\n",
    "        cluster.train(dataRDD, args.epochs)\n",
    "    else:\n",
    "        print(\"servers for inference\")\n",
    "        labelRDD = cluster.inference(dataRDD)\n",
    "        print(\"--------------------------------received labelRDD-------------------------\")\n",
    "        labelRDD.saveAsTextFile(args.output)\n",
    "    cluster.shutdown()\n",
    "    print(\"Finnished, cluster shutdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d5093d0faa19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'spark'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'spark_setup_cluster_training()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-122>\u001b[0m in \u001b[0;36mspark\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"ENCOUNTERED AN INTERNAL ERROR: {}\\n\\tTraceback:\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# Do not log! as some messages may contain private client information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/kernels/kernelmagics.pyc\u001b[0m in \u001b[0;36mspark\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/magics/sparkmagicsbase.pyc\u001b[0m in \u001b[0;36mexecute_spark\u001b[1;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.pyc\u001b[0m in \u001b[0;36mrun_command\u001b[1;34m(self, command, client_name)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36m_get_statement_output\u001b[1;34m(self, session, statement_id)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/livysession.pyc\u001b[0m in \u001b[0;36msleep\u001b[1;34m(self, retries)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark_setup_cluster_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
