{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2007</td><td>application_1512575073636_0537</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1512575073636_0537/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop9:8042/node/containerlogs/container_e26_1512575073636_0537_01_000001/har_1__kimham00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from hops import hdfs\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from tensorflowonspark import TFCluster\n",
    "from hops import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_path = \"/Projects/\" + hdfs.project_name()\n",
    "\n",
    "TRAIN_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/features\"\n",
    "TRAIN_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/labels\"\n",
    "TEST_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/features\"\n",
    "TEST_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/labels\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_args(num_executors):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', \"--cluster\", action='store_true', default=False)\n",
    "    parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\n",
    "    parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "    parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "    parser.add_argument(\"-f\", \"--features\", help=\"HDFS path to features in parallelized format\", default=TRAIN_FEATURES_PATH)\n",
    "    parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to labels in parallelized format\", default=TRAIN_LABELS_PATH)\n",
    "    parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/inference\", default=project_path + \"/HAR_Dataset/saved_model\")\n",
    "    parser.add_argument(\"-r\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=project_path + \"/HAR_Dataset/predictions\")\n",
    "    parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=100000)\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", help=\"number of records per batch\", type=int, default=100)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=100)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def map_fun(args, ctx):\n",
    "    \"\"\"Training/Inference Function executed by parameter-servers and workers in distributed TFOS\"\"\"\n",
    "    NUM_FEATURES = 8\n",
    "    NUM_CLASSES = 7\n",
    "\n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"%d: \" % worker_num)\n",
    "        print(arg)\n",
    "\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "    from hops import tensorboard\n",
    "\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    print_log(worker_num, \"task_index: {0}, job_name {1}, cluster_spec: {2}\".format(task_index, job_name, cluster_spec))\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 10)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    print_log(worker_num, \"batch_size: {0}\".format(batch_size))\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def read_csv_examples(feature_dir, label_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "        \n",
    "        # Setup queue of csv feature filenames\n",
    "        tf_record_pattern = os.path.join(feature_dir, 'part-*')\n",
    "        features = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"features: {0}\".format(features))\n",
    "        feature_queue = tf.train.string_input_producer(features, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                       name=\"feature_queue\")\n",
    "\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"labels: {0}\".format(labels))\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                     name=\"label_queue\")\n",
    "\n",
    "        # Setup reader for feature queue\n",
    "        feature_reader = tf.TextLineReader(name=\"feature_reader\")\n",
    "        _, feat_csv = feature_reader.read(feature_queue)\n",
    "        feature_defaults = [[1.0] for col in range(NUM_FEATURES)]\n",
    "        feature = tf.stack(tf.decode_csv(feat_csv, feature_defaults), name=\"input_features\")\n",
    "        print_log(worker_num, \"feature: {0}\".format(feature))\n",
    "\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [tf.constant([], dtype=tf.int64)]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults), name = \"input_labels\")\n",
    "        print_log(worker_num, tf.shape(label))\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([feature, label], batch_size, num_threads=10, name=\"batch_csv\")\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        print_log(worker_num, \"Parameter Server Joining\")\n",
    "        server.join()\n",
    "\n",
    "    elif job_name == \"worker\":\n",
    "        print_log(worker_num, \"worker {0} starting\")\n",
    "\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "                worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "                cluster=cluster)):\n",
    "\n",
    "            def build_graph(x):\n",
    "                \"\"\"Builds the computational graph of the model\"\"\"\n",
    "                W = tf.Variable(\"W_1\", tf.zeros([NUM_FEATURES, NUM_CLASSES]))\n",
    "                tf.summary.histogram(\"input_weights\", W) #for tensorboard\n",
    "                b = tf.Variable(\"bias_weights\", tf.zeros([NUM_CLASSES]))\n",
    "                tf.summary.histogram(\"bias_weights\", b) #for tensorboard\n",
    "                logits = tf.matmul(x, W) + b\n",
    "                return logits\n",
    "\n",
    "            def define_optimizer(logits, labels):\n",
    "                \"\"\"Defines the optimizer of the model and calculates step, loss, prediction, accuracy\"\"\"\n",
    "                #Global step to keep track of how long training have proceeded, \n",
    "                #incremented by one for each gradient computation\n",
    "                global_step = tf.Variable(\"global_step\",0)\n",
    "                # Define loss and optimizer\n",
    "                cross_entropy = tf.reduce_mean(\n",
    "                    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(labels, [-1]), logits=logits))\n",
    "                tf.summary.scalar(\"loss\", cross_entropy) #for tensorboard\n",
    "                train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy, global_step=global_step)\n",
    "                prediction = tf.argmax(tf.nn.softmax(logits), 1, name=\"prediction\")\n",
    "                # Test trained model\n",
    "                correct_prediction = tf.equal(tf.argmax(logits, 1), labels)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "                tf.summary.scalar(\"acc\", accuracy) #for tensorboard\n",
    "                return train_step, accuracy, cross_entropy, global_step, prediction\n",
    "\n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            features = TFNode.hdfs_path(ctx, args.features) #input csv files\n",
    "            labels = TFNode.hdfs_path(ctx, args.labels) #input csv files\n",
    "            \n",
    "            x, y = read_csv_examples(features, labels, 100, num_epochs, index, workers)\n",
    "            logits = build_graph(x)\n",
    "            training_step, accuracy, cross_entropy_loss, global_step, pred = define_optimizer(logits, y)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "        logdir = tensorboard.logdir()\n",
    "        print_log(worker_num, \"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "        if job_name == \"worker\" and task_index == 0:\n",
    "            summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "        if args.mode == \"train\":\n",
    "            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         init_op=init_op,\n",
    "                                         summary_op=None,\n",
    "                                         summary_writer=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=10)\n",
    "        else:\n",
    "            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         summary_op=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=0)\n",
    "\n",
    "        output_dir = TFNode.hdfs_path(ctx, args.output)\n",
    "        output_file = tf.gfile.Open(\"{0}/part-{1:05d}\".format(output_dir, worker_num), mode='w')\n",
    "        \n",
    "        model_dir = TFNode.hdfs_path(ctx, args.model)\n",
    "        \n",
    "        # The supervisor takes care of session initialization, restoring from\n",
    "        # a checkpoint, and closing when done or an error occurs.\n",
    "        with sv.managed_session(server.target) as sess:\n",
    "            print_log(worker_num, \"session ready, starting training\")\n",
    "\n",
    "            # Loop until the supervisor shuts down or maximum steps have completed.\n",
    "            step = 0\n",
    "            count = 0\n",
    "            while not sv.should_stop() and step < args.steps:\n",
    "                # Run a training step asynchronously.\n",
    "                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "                # perform *synchronous* training.\n",
    "\n",
    "                if args.mode == \"train\":\n",
    "                    _, summary, step = sess.run([training_step, summary_op, global_step])\n",
    "                    # logging.info accuracy and save model checkpoint to HDFS every 100 steps\n",
    "                    if (step % 100 == 0):\n",
    "                        acc = sess.run(accuracy)\n",
    "                        print_log(worker_num, \"step: {0}, acc: {1}\".format(step, acc))\n",
    "\n",
    "                    if sv.is_chief:\n",
    "                        summary_writer.add_summary(summary, step)\n",
    "                else:  # args.mode == \"inference\"\n",
    "                    label, preds, acc = sess.run([labels, pred, accuracy])\n",
    "                    for i in range(len(label)):\n",
    "                        count += 1\n",
    "                        output_file.write(\"{0} {1}\\n\".format(label[i], pred[i]))\n",
    "                    print(\"count: {0}\".format(count))\n",
    "\n",
    "            if args.mode == \"inference\":\n",
    "                output_file.close()\n",
    "            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n",
    "            # run inference and request stop before the other workers even start/sync their sessions.\n",
    "            if task_index == 0:\n",
    "                time.sleep(60)\n",
    "            \n",
    "            if sv.is_chief:\n",
    "                save_path = saver.save(sess, model_dir)\n",
    "                print_log(worker_num, \"Model saved in file: {}\".format(save_path))\n",
    "\n",
    "            # Ask for all the services to stop.\n",
    "            print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "            sv.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark Cluster Setup For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_setup_cluster_training():\n",
    "    from hops import tensorboard\n",
    "    num_executors = util.num_executors(spark)\n",
    "    num_ps = util.num_param_servers(spark)\n",
    "    args = parse_args(num_executors)\n",
    "    cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\n",
    "    cluster.shutdown()\n",
    "    print(\"Finnished, cluster shutdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_setup_cluster_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
