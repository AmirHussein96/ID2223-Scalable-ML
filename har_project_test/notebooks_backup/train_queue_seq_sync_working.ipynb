{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2451</td><td>application_1512575073636_1010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1512575073636_1010/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop7:8042/node/containerlogs/container_e26_1512575073636_1010_01_000001/har_1__kimham00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from hops import hdfs\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from tensorflowonspark import TFCluster\n",
    "from hops import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_path = \"/Projects/\" + hdfs.project_name()\n",
    "\n",
    "TRAIN_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/features\"\n",
    "TRAIN_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/train/labels\"\n",
    "TEST_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data/test/features\"\n",
    "TEST_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data/test/labels\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_args(num_executors):\n",
    "    \"setup parser if running from CLI\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c', \"--cluster\", action='store_true', default=False)\n",
    "    parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\n",
    "    parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "    parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "    parser.add_argument(\"-f\", \"--features\", help=\"HDFS path to features in parallelized format\", default=TRAIN_FEATURES_PATH)\n",
    "    parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to labels in parallelized format\", default=TRAIN_LABELS_PATH)\n",
    "    parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/inference\", default=project_path + \"/HAR_Dataset/saved_model/saved_model\")\n",
    "    parser.add_argument(\"-r\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=project_path + \"/HAR_Dataset/predictions\")\n",
    "    parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=100000)\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", help=\"number of records per batch\", type=int, default=100)\n",
    "    parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"-lr\", \"--learningrate\", help=\"number of epochs\", type=float, default=0.00025)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def training_args(args):\n",
    "    \"helper function for default train parameters\"\n",
    "    args.features = TRAIN_FEATURES_PATH\n",
    "    args.labels = TRAIN_LABELS_PATH\n",
    "    args.mode = \"train\"\n",
    "    args.steps = 100000\n",
    "    args.batch_size = 100\n",
    "    args.epochs=100\n",
    "    return args\n",
    "    \n",
    "def test_args(args):\n",
    "    \"helper function for default test parameters\"\n",
    "    args.features = TEST_FEATURES_PATH\n",
    "    args.labels = TEST_LABELS_PATH\n",
    "    args.mode = \"inference\"\n",
    "    args.steps = 100000\n",
    "    args.batch_size = 100\n",
    "    args.epochs=100\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def map_fun(args, ctx):\n",
    "    \"\"\"Training/Inference Function executed by parameter-servers and workers in distributed TFOS\"\"\"\n",
    "    NUM_FEATURES = 3\n",
    "    NUM_CLASSES = 7\n",
    "    SEQUENCE_SIZE = 10\n",
    "    NUM_HIDDEN_UNITS = 64\n",
    "\n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"Worker {0}: {1}\".format(worker_num, arg))\n",
    "\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import getpass\n",
    "    import math\n",
    "    import numpy\n",
    "    import os\n",
    "    import signal\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    # Used to get TensorBoard logdir for TensorBoard that show up in HopsWorks\n",
    "    from hops import tensorboard\n",
    "\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    print_log(worker_num, \"task_index: {0}, job_name {1}, cluster_spec: {2}\".format(task_index, job_name, cluster_spec))\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    print_log(worker_num, \"batch_size: {0}\".format(batch_size))\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "    \n",
    "    def read_csv_features(feature_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of csv feature filenames\n",
    "        tf_record_pattern = os.path.join(feature_dir, 'part-*')\n",
    "        features = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"features: {0}\".format(features))\n",
    "        feature_queue = tf.train.string_input_producer(features, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                       name=\"feature_queue\")\n",
    "        # Setup reader for feature queue\n",
    "        feature_reader = tf.TextLineReader(name=\"feature_reader\")\n",
    "        _, feat_csv = feature_reader.read(feature_queue)\n",
    "        feature_defaults = [[1.0] for col in range(NUM_FEATURES)]\n",
    "        feature = tf.stack(tf.decode_csv(feat_csv, feature_defaults), name=\"input_features\")\n",
    "        print_log(worker_num, \"features: {0}, shape: {1}\".format(feature, feature.shape))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([feature], batch_size, num_threads=10, name=\"batch_csv\")\n",
    "\n",
    "    def read_csv_labels(label_dir, batch_size=10, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"labels: {0}\".format(labels))\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                     name=\"label_queue\")\n",
    "\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [tf.constant([], dtype=tf.int64)]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults), name = \"input_labels\")\n",
    "        print_log(worker_num, tf.shape(label))\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([label], batch_size, num_threads=10, name=\"label_batch_csv\")\n",
    "    \n",
    "    if job_name == \"ps\":\n",
    "        print_log(worker_num, \"Parameter Server Joining\")\n",
    "        server.join()\n",
    "\n",
    "    elif job_name == \"worker\":\n",
    "        print_log(worker_num, \"worker {0} starting\")\n",
    "\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "                worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "                cluster=cluster)):\n",
    "\n",
    "            def build_graph(X):\n",
    "                \"\"\"Builds the computational graph of the model\"\"\"\n",
    "                \n",
    "                print_log(worker_num, \"build graph, input shape: {0}\".format(X.shape))\n",
    "                \n",
    "                W = {\n",
    "                    'hidden': tf.Variable(tf.random_normal([NUM_FEATURES, NUM_HIDDEN_UNITS])),\n",
    "                    'output': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS, NUM_CLASSES]))\n",
    "                }\n",
    "                biases = {\n",
    "                        'hidden': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS], mean=1.0)),\n",
    "                        'output': tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "                }\n",
    "    \n",
    "                #X = tf.transpose(inputs, [1, 0, 2])\n",
    "                #X = tf.reshape(X, [-1, N_FEATURES])\n",
    "                hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])\n",
    "                hidden = tf.split(hidden, SEQUENCE_SIZE, 0)\n",
    "\n",
    "                # Stack 2 LSTM layers\n",
    "                lstm_layers = [tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\n",
    "                lstm_layers = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "                outputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "\n",
    "                # Get output for the last time step\n",
    "                lstm_last_output = outputs[-1]\n",
    "                logits = tf.matmul(lstm_last_output, W['output']) + biases['output']\n",
    "                return logits\n",
    "\n",
    "            def define_optimizer(logits, labels, LEARNING_RATE):\n",
    "                \"\"\"Defines the optimizer of the model and calculates step, loss, prediction, accuracy\"\"\"\n",
    "                \n",
    "                print_log(worker_num, \"define optimizer, labels shape: {0}, logits shape: {1}\".format(labels.shape, logits.shape))\n",
    "                \n",
    "                #Global step to keep track of how long training have proceeded, \n",
    "                #incremented by one for each gradient computation\n",
    "                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "                \n",
    "                softmax_prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                prediction = tf.argmax(softmax_prediction, 1)\n",
    "                \n",
    "                #L2 Regularization\n",
    "                L2_LOSS = 0.0015\n",
    "                l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "                \n",
    "                # Define loss and optimizer\n",
    "                cross_entropy = tf.reduce_mean(\n",
    "                    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.reshape(labels, [-1]), \n",
    "                        logits=logits)) + l2\n",
    "                tf.summary.scalar(\"loss\", cross_entropy) #for tensorboard\n",
    "                \n",
    "                opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "                opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=num_workers,\n",
    "                                                     total_num_replicas=num_workers)\n",
    "                \n",
    "                train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "                \n",
    "                # Test trained model\n",
    "                correct_prediction = tf.equal(prediction, labels)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "                tf.summary.scalar(\"acc\", accuracy) #for tensorboard\n",
    "                sync_replicas_hook = opt.make_session_run_hook(task_index == 0)\n",
    "                chief_queue_runner = opt.get_chief_queue_runner()\n",
    "                return train_step, accuracy, cross_entropy, global_step, prediction, correct_prediction, sync_replicas_hook, chief_queue_runner\n",
    "\n",
    "            LEARNING_RATE = args.learningrate\n",
    "            \n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            features = TFNode.hdfs_path(ctx, args.features) #input csv files\n",
    "            labels = TFNode.hdfs_path(ctx, args.labels) #input csv files\n",
    "            \n",
    "            x = read_csv_features(features, batch_size, num_epochs, index, workers)\n",
    "            y = read_csv_labels(labels, batch_size/SEQUENCE_SIZE, num_epochs, index, workers)\n",
    "            print_log(worker_num, \"shape: {0}, {1}\".format(x.shape, y.shape))\n",
    "            \n",
    "            logits = build_graph(x)\n",
    "            training_step, accuracy, loss, global_step, pred, correct_prediction, sync_replicas_hook, chief_queue_runner = define_optimizer(logits, y, LEARNING_RATE)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "        logdir = tensorboard.logdir()\n",
    "        print_log(worker_num, \"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "        if job_name == \"worker\" and task_index == 0:\n",
    "            summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "        if args.mode == \"train\":\n",
    "            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         init_op=init_op,\n",
    "                                         summary_op=None,\n",
    "                                         summary_writer=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=10)\n",
    "        else:\n",
    "            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         summary_op=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=0)\n",
    "\n",
    "        output_dir = TFNode.hdfs_path(ctx, args.output)\n",
    "        output_file = tf.gfile.Open(\"{0}/part-{1:05d}\".format(output_dir, worker_num), mode='w')\n",
    "        \n",
    "        model_dir = TFNode.hdfs_path(ctx, args.model)\n",
    "        \n",
    "        # The supervisor takes care of session initialization, restoring from\n",
    "        # a checkpoint, and closing when done or an error occurs.\n",
    "        with sv.managed_session(server.target) as sess:\n",
    "            \"\"\"Asynchronous SGD training with supervisor\"\"\"\n",
    "            if sv.is_chief:\n",
    "                    sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "            \n",
    "            print_log(worker_num, \"session ready, starting training\")\n",
    "            # Loop until the supervisor shuts down or maximum steps have completed.\n",
    "            step = 0\n",
    "            count = 0\n",
    "            #sv.start_queue_runners(sess)\n",
    "            while not sv.should_stop() and step < args.steps:\n",
    "                # Run a training step asynchronously.\n",
    "                if args.mode == \"train\":\n",
    "                    _, summary, step = sess.run([training_step, summary_op, global_step])\n",
    "                    # logging.info accuracy and save model checkpoint to HDFS every 100 steps\n",
    "                    acc = sess.run(accuracy)\n",
    "                    print_log(worker_num, \"step: {0}, acc: {1}\".format(step, acc))\n",
    "                    #if (step % 100 == 0):\n",
    "                        #preds, acc, losss = sess.run([pred, accuracy, loss])\n",
    "                        #print_log(worker_num, \"step: {0}, acc: {1}, loss: {2}\".format(step, acc, losss))\n",
    "                        #xx,yy,corr_pred = sess.run([x, y, correct_prediction])\n",
    "                        #print_log(worker_num, \"x: {0}, y: {1}\".format(xx, yy))\n",
    "                        #print_log(worker_num, \"pred: \\n {2} \\n corr_pred: \\n {0}, \\n label: \\n {1} \\n\".format(corr_pred, yy, preds))\n",
    "                        \n",
    "                    if sv.is_chief:\n",
    "                        summary_writer.add_summary(summary, step)\n",
    "                else:  # args.mode == \"inference\"\n",
    "                    print_log(worker_num, \"doing inference\")\n",
    "                    label, preds, acc = sess.run([labels, pred, accuracy])\n",
    "                    for i in range(len(label)):\n",
    "                        count += 1\n",
    "                        output_file.write(\"{0} {1}\\n\".format(label[i], pred[i]))\n",
    "                    print_log(worker_num, \"count: {0}\".format(count))\n",
    "\n",
    "            if args.mode == \"inference\":\n",
    "                output_file.close()\n",
    "                # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n",
    "                # run inference and request stop before the other workers even start/sync their sessions.\n",
    "                if task_index == 0:\n",
    "                    time.sleep(60)\n",
    "            \n",
    "            if sv.is_chief:\n",
    "                save_path = saver.save(sess, model_dir)\n",
    "                print_log(worker_num, \"Model saved in file: {}\".format(save_path))\n",
    "                summ = tf.summary.FileWriter(model_dir, graph=tf.get_default_graph())\n",
    "                summ.flush()\n",
    "\n",
    "            # Ask for all the services to stop.\n",
    "            print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "            sv.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark Cluster Setup For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_setup_cluster_training():\n",
    "    \"\"\"Start the cluster training with given parameters\"\"\"\n",
    "    from hops import tensorboard\n",
    "    num_executors = util.num_executors(spark)\n",
    "    num_ps = util.num_param_servers(spark)\n",
    "    args = parse_args(num_executors)\n",
    "    args = training_args(args)\n",
    "    #args = test_args(args)\n",
    "    cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\n",
    "    cluster.shutdown()\n",
    "    print(\"Finnished, cluster shutdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d5093d0faa19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'spark'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'spark_setup_cluster_training()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-122>\u001b[0m in \u001b[0;36mspark\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"ENCOUNTERED AN INTERNAL ERROR: {}\\n\\tTraceback:\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# Do not log! as some messages may contain private client information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/kernels/kernelmagics.pyc\u001b[0m in \u001b[0;36mspark\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/magics/sparkmagicsbase.pyc\u001b[0m in \u001b[0;36mexecute_spark\u001b[1;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.pyc\u001b[0m in \u001b[0;36mrun_command\u001b[1;34m(self, command, client_name)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36m_get_statement_output\u001b[1;34m(self, session, statement_id)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/livysession.pyc\u001b[0m in \u001b[0;36msleep\u001b[1;34m(self, retries)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark_setup_cluster_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
